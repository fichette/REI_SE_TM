{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import os\n",
    "from gensim import corpora, models\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "import re\n",
    "import pickle\n",
    "from pattern.text.fr import parse\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fonction qui permet de transformer les mots d'un texte en lemmes\n",
    "def lemmatization(content_file):\n",
    "    content_file = parse(content_file, relations=True, lemmata=True).split(\" \")\n",
    "    res = [elt.split(\"/\")[5] for elt in content_file]\n",
    "    return \" \".join(res)\n",
    "\n",
    "\n",
    "\n",
    "# Construction d'un stemmer pour le français\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "\n",
    "\n",
    "# Recuperation de la liste des stopwords depuis un fichier \n",
    "import codecs\n",
    "file_stopwords = codecs.open(\"frenchST.txt\",\"r\", encoding=\"utf-8\")\n",
    "stopwords = file_stopwords.read().split(\"\\n\")\n",
    "file_stopwords.close()\n",
    "\n",
    "\n",
    "# Creation d'un set de symboles de ponctuation qui servira a la suppression de la ponctuation\n",
    "ponctuation = set(string.punctuation)\n",
    "\n",
    "\n",
    "# Fonction qui permet de supprimer les mots peu frequents d'un document\n",
    "# La notion de \"peu frequent\" est defini par le parametre \"min_freq\"\n",
    "# Tout mot dont la frequence dans le document est < à min_freq est supprime\n",
    "def remove_infrequent_words(content_file, min_freq):\n",
    "    content_file = np.array(content_file.split())\n",
    "    # creation d'une distribution de frequences sur le contenu du fichier en utilisant nltk\n",
    "    freq_dist = nltk.FreqDist(content_file) \n",
    "    # On obtient un dictionnaire des mots du fichier avec leur frequence\n",
    "    for elt in freq_dist:  # elt represente une cle, c'est le mot \n",
    "        if freq_dist[elt] < min_freq: #freq_dist[elt] est la frequence du mot elt\n",
    "            content_file = np.delete(content_file, np.where(content_file == elt)[0])\n",
    "    return ' '.join(content_file)\n",
    "\n",
    "\n",
    "# Fonction qui permet de raciniser les mots d'un texte\n",
    "def stemming(content_file):\n",
    "    content_file = content_file.split()\n",
    "    for word in range(len(content_file)):\n",
    "        content_file[word] = stemmer.stem(content_file[word].decode(\"utf-8\"))\n",
    "    return ' '.join(content_file)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data_file(content_file):\n",
    "    content_file = content_file.lower()  # Faire passer les caracteres du texte du majuscule au minuscule \n",
    "    # Supprimer quelques caracteres speciaux non traites avec le set de ponctuation\n",
    "    content_file = content_file.replace('’', \"\")\n",
    "    content_file = content_file.replace('«', \"\")\n",
    "    content_file = content_file.replace('»', \"\")\n",
    "    content_file = content_file.replace('—', \"\")\n",
    "    content_file = content_file.replace('\\'', \"\")\n",
    "    content_file = content_file.replace('©', \"\")\n",
    "    content_file = content_file.replace('–', \"\")\n",
    "    content_file = content_file.replace('¿', )\n",
    "    # Supprimer les nombres\n",
    "    content_file = re.sub(\"[0-9]+\", \"\", content_file)\n",
    "    # Supprimer la ponctuation \n",
    "    content_file = ''.join(char for char in content_file if char not in ponctuation) \n",
    "    # Appel de la fonction qui supprime les mots peu frequents, ici avec frequence minimale de 2\n",
    "    content_file = remove_infrequent_words(content_file, 2)\n",
    "    #content_file = stemming(content_file)\n",
    "    return content_file\n",
    "\n",
    "\n",
    "\n",
    "#_____________________________________Preparation du corpus_________________________________________________\n",
    "# Initialisation du chemin sur le repertoir contenant le corpus\n",
    "chemin_corpus = os.getcwd()+\"/Subset/\"\n",
    "# Recuperer la liste des sous dossiers\n",
    "list_dir = os.listdir(chemin_corpus)\n",
    "infopath = [] # Structure pour stocker les chemins des fichiers (necessaire pour la visualisation)\n",
    "infolength=[] # Structure pour stocker les longueurs des fichiers (necessaire pour la visualisation)\n",
    "texts  =[] # liste des documents apres pre-traitement\n",
    "\n",
    "for dir_annee in list_dir:\n",
    "    sub_dirs_annee = os.listdir(chemin_corpus+dir_annee)   #recuperer les sous dossiers correspondants aux annees\n",
    "    for sub_dir in sub_dirs_annee:\n",
    "        sub_sub_dirs = os.listdir(chemin_corpus+dir_annee+\"/\"+sub_dir) #recuperer les sous dossiers correspondants aux mois\n",
    "        for sub_sub_dir in sub_sub_dirs:\n",
    "            files = os.listdir(chemin_corpus+dir_annee+\"/\"+sub_dir+\"/\"+sub_sub_dir) # recupere les sous dossiers correspondants aux jours\n",
    "            for f in files: # pour chaque fichier...\n",
    "                if os.stat(chemin_corpus+dir_annee+\"/\"+sub_dir+\"/\"+sub_sub_dir+\"/\"+f).st_size!=0: # Verifier que le fichier n'est pas vide\n",
    "                    open_file = codecs.open(chemin_corpus+dir_annee+\"/\"+sub_dir+\"/\"+sub_sub_dir+\"/\"+f, \"r\", encoding=\"utf-8\")\n",
    "                    content_file = open_file.read()\n",
    "                    if detect(content_file)== \"fr\": # Confirmer que la langue c'est bien du français\n",
    "                        content_file = lemmatization(content_file) # Appliquer la lemmatization\n",
    "                        content_file = preprocess_data_file(content_file) # Appliquer d'autres pre-traitements\n",
    "                        content_file2 = []\n",
    "                        # Supprimer les stop words\n",
    "                        # La boucle a ete formulee ainsi et non en (if word in ...) a cause de problemes d'encodage \n",
    "                        for word in content_file.lower().split():\n",
    "                            existe = False\n",
    "                            for stopword in stopwords:\n",
    "                                if word == stopword:\n",
    "                                    existe = True\n",
    "                                    break\n",
    "                            if existe != True:\n",
    "                                content_file2.append(word)\n",
    "\n",
    "                        content_file = content_file2 # contenu du fichier pre-traité\n",
    "\n",
    "                        # Eliminer les fichiers qui après pre-traitement deviennent vides ou trop petits\n",
    "                        if len(content_file) > 5:\n",
    "                            texts.append(content_file)\n",
    "                            # Ajouter le chemin et la longueur du fichier dans les structures precedentes\n",
    "                            infopath.append(chemin_corpus+dir_annee+\"/\"+sub_dir+\"/\"+sub_sub_dir+\"/\"+f)\n",
    "                            infolength.append(len(content_file))\n",
    "                    open_file.close()\n",
    "#___________________________________________________________________________________________________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Supprimer les accents qui posent problemes en normalisant a l'unicode\n",
    "import unicodedata\n",
    "texts = [[unicodedata.normalize('NFD', word).encode('ascii', 'ignore') for word in text] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enregistrer texts, infopath et infolength pour ne pas reexecuter le code a chaque fois\n",
    "pickle.dump(texts, open(os.getcwd()+\"/10000/texts.obj\", \"wb\"))\n",
    "pickle.dump(infopath, open(os.getcwd()+\"/10000/infopath.obj\", \"wb\"))\n",
    "pickle.dump(infolength, open(os.getcwd()+\"/10000/infolength.obj\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparation des INPUT du LDA\n",
    "# creation d'un dictionnaire qui attribu a chaque mot de la collection un identifiant unique\n",
    "dictionary = corpora.Dictionary(texts) \n",
    "\n",
    "# mapper le corpus de la structure basique a une structure de liste de document \n",
    "# chaque document est une liste de tuples (mot, nombre d'occurence dans le document)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts] #term_frequency\n",
    "\n",
    "# Appel de lda Multicore plutot que le lda basique\n",
    "# lda Multicore permet une execution plus rapide en specifiant le nombre de workers \n",
    "# chunksize : lenombre de documents qui peuvent etre charges en memoire\n",
    "# num_topics: nombre de topics\n",
    "# passes : nombre de passages d'apprentissage sur le corpus que l'on veut effectuer\n",
    "lda = models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=50, workers = 2,chunksize=10000, passes=1)\n",
    "\n",
    "#lda.update(other_corpus) # Fonction qui permet d'enrichir le lda avec d'autres donnees\n",
    "\n",
    "# Visualisation des top topics si necessaire\n",
    "#top = lda.top_topics(corpus=corpus)\n",
    "#for top_topic in top:\n",
    "#    print top_topic\n",
    "\n",
    "\n",
    "# Enregistrer le output LDA\n",
    "#pickle.dump(lda, open(os.getcwd()+\"/lda_output\", \"wb\"))\n",
    "\n",
    "\n",
    "# Recharger le output LDA\n",
    "#lda = pickle.load(open(os.getcwd()+\"/lda_output\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#______________Preparation des input pour la visualisation______________________\n",
    "# Distribution des probabilites qu'un document i (lignes) ait un topic j (colonnes)\n",
    "def doc_topic_dists(corpus, lda):\n",
    "    doc_topic_dists = []\n",
    "    for doc in range(len(corpus)):\n",
    "        list_topic_proba = []\n",
    "        temp = lda.get_document_topics(corpus[doc], minimum_probability=0)\n",
    "        for topic, proba in temp:\n",
    "            list_topic_proba.append(proba)\n",
    "        doc_topic_dists.append(list_topic_proba)\n",
    "    return doc_topic_dists\n",
    "#doc_topic_dists = doc_topic_dists(corpus, lda) \n",
    "\n",
    "# Distribution des probabilites qu'un topic i (lignes) contienne un term j (colonnes)\n",
    "def topic_term_dists(lda, num_topics, len_vocab):\n",
    "    topic_term_dists = []\n",
    "    for topic in range(num_topics):\n",
    "        list_term_proba = [0]*len_vocab\n",
    "        temp = lda.get_topic_terms(topic)\n",
    "        for term, proba in temp:\n",
    "            list_term_proba[term] = proba\n",
    "        topic_term_dists.append(list_term_proba)\n",
    "    return topic_term_dists\n",
    "# topic_term_dists = topic_term_dists(lda, 5, len(dictionary.token2id))\n",
    "\n",
    "\n",
    "#Pour avoir tout le vocabulaire ou chaque mot est represente par son identifiant\n",
    "def get_vocabularyIDs(dictionary_tokens):\n",
    "    vocabulary = []\n",
    "    for token in dictionary_tokens:\n",
    "        vocabulary.append(dictionary_tokens[token])\n",
    "    return vocabulary\n",
    "# vocabularyIDs = get_vocabularyIDs(dictionary.token2id)  \n",
    "\n",
    "\n",
    "#Pour avoir tout le vocabulaire ou chaque mot est represente dans sa forme normale\n",
    "def get_vocabularyAlpha(dictionary_tokens):\n",
    "    vocabulary = []\n",
    "    for token in dictionary_tokens:\n",
    "        vocabulary.append(dictionary_tokens[token])\n",
    "    return vocabulary\n",
    "# vocabulary_alpha = get_vocabularyAlpha(dictionary.id2token) \n",
    "\n",
    "\n",
    "# Recuperation de la longueur des fichiers\n",
    "def doc_lengths(texts):\n",
    "    doc_lengths = []\n",
    "    for text in texts:\n",
    "        doc_lengths.append(len(text))\n",
    "    return doc_lengths\n",
    "#____________________________________________________________________________________\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
