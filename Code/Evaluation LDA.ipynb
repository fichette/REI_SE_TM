{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import os\n",
    "from gensim import corpora, models\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "import re\n",
    "import pickle\n",
    "from pattern.text.fr import parse\n",
    "import math\n",
    "\n",
    "from whoosh import query\n",
    "import whoosh.index as index\n",
    "from whoosh.fields import Schema, ID, TEXT\n",
    "from whoosh.index import create_in, open_dir\n",
    "from whoosh.query import Term, SpanNear, And\n",
    "\n",
    "import sys  \n",
    "\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%install_ext autotime.py\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution du topic modeling\n",
    "Importer les données des pickles: la structure texts contenant la liste de listes de mots, infopath contenant les noms et chemins des documents et infolength contenant la tailel des documents\n",
    "Construction du dictionnaire et de la structure corpus puis execution du LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = pickle.load(open(os.getcwd()+\"/5000/texts.obj\", \"rb\"))\n",
    "infopath= pickle.load(open(os.getcwd()+\"/5000/infopath.obj\", \"rb\"))\n",
    "infolength= pickle.load(open(os.getcwd()+\"/5000/infolength.obj\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata #remplacer les accents \n",
    "texts = [[unicodedata.normalize('NFD', word).encode('ascii', 'ignore') for word in text] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#construction du dictionnaire pour LDA\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts] #term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics=30 #nombre de topics\n",
    "len_vocab=len(dictionary.token2id) #nombre de mots dans le vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Execution de LDA\n",
    "lda = models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, workers =2,chunksize=10000, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'faire', 0.011940197752507507),\n",
       " (u'france', 0.0088737718992821565),\n",
       " (u'a', 0.0085395776033475555),\n",
       " (u'grand', 0.0073334296902422807),\n",
       " (u'pouvoir', 0.0070702249485926141),\n",
       " (u'devoir', 0.0060057841170735116),\n",
       " (u'europeen', 0.0055063935114626769),\n",
       " (u'ministre', 0.0052916304673419458),\n",
       " (u'an', 0.0047609720792872717),\n",
       " (u'pays', 0.0047118960566896444)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#affichage des topics avec fréquences\n",
    "lda.print_topics(num_topics=4, num_words=10)\n",
    "#lda.show_topic(29, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contruction index\n",
    "Nous construisons un index sur l'ensemble des documents afin de faciliter la tâche d'estimation des probabilités liées aux mots et topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creation de l'index sur l'ensemble du corpus\n",
    "def create_index(texts, indexPath = \"index\"):\n",
    "    schema = Schema(content=TEXT(stored=True),nid=ID(stored=True))\n",
    "    if not os.path.exists(indexPath):\n",
    "        os.mkdir(indexPath)\n",
    "    index = create_in(indexPath, schema)\n",
    "    writer = index.writer()\n",
    "    for i in range(len(texts)):\n",
    "        writer.add_document(content=texts[i],nid=unicode(i))\n",
    "    writer.commit()\n",
    "create_index(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx=index.open_dir(\"index\") #load index\n",
    "searcher=idx.searcher() #open index searcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperation des distributions\n",
    "Nous récupérons les distributions de probabilités dans des matrices: Distributoons des topics sur les documents et distributions des mots sur les topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Matrice Doc Topic Distributions\n",
    "def get_doc_topic_dists(corpus, lda, num_topics):\n",
    "    doc_topic_dists = []\n",
    "    for doc in range(len(corpus)):\n",
    "        list_topic_proba = [0]*num_topics\n",
    "        temp = lda.get_document_topics(corpus[doc], minimum_probability=0)\n",
    "        for topic, proba in temp:\n",
    "            list_topic_proba[topic]=proba\n",
    "        doc_topic_dists.append(list_topic_proba)\n",
    "    return doc_topic_dists\n",
    "\n",
    "#Matrice Topic Doc Distributions\n",
    "def get_topic_doc_dists(doc_topic_dists):\n",
    "    topic_doc_dists=[]\n",
    "    for k in range(len(doc_topic_dists[0])):\n",
    "        tmp=[]\n",
    "        for d in range(len(doc_topic_dists)):\n",
    "            tmp.append(doc_topic_dists[d][k])\n",
    "        topic_doc_dists.append(tmp)\n",
    "    return topic_doc_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic_dists=get_doc_topic_dists(corpus, lda, num_topics)\n",
    "topic_doc_dists=get_topic_doc_dists(doc_topic_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matrice Topic Mot distributions\n",
    "def get_topic_term_dists(lda, num_topics, len_vocab):\n",
    "    topic_term_dists = []\n",
    "    for topic in range(num_topics):\n",
    "        list_term_proba = [0]*len_vocab\n",
    "        temp = lda.get_topic_terms(topic,topn=len_vocab)\n",
    "        for term, proba in temp:\n",
    "            list_term_proba[term] = proba\n",
    "        topic_term_dists.append(list_term_proba)\n",
    "    return topic_term_dists\n",
    "\n",
    "# matrice Mots Topic distributions\n",
    "def get_term_topic_dists(topic_term_dists):\n",
    "    term_topic_dists=[]\n",
    "    for w in range(len(topic_term_dists[0])):\n",
    "        tmp=[]\n",
    "        for k in range(len(topic_term_dists)):\n",
    "            tmp.append(topic_term_dists[k][w])\n",
    "        term_topic_dists.append(tmp)\n",
    "    return term_topic_dists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_term_dists=get_topic_term_dists(lda, num_topics, len_vocab)\n",
    "term_topic_dists=get_term_topic_dists(topic_term_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information mutuelle et entropie\n",
    "Dans cette partie nous définissons les fonctions qui calculent l'information mutuelle (pmi, npmi, pmi-average) et l'entropie (relevance ou pertinence) des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PMI for two words\n",
    "def pmi(searcher, w1, w2,epsilon=0.1, window=10):\n",
    "    doc_count=float(searcher.doc_count())\n",
    "    t1 = query.Term(\"content\", w1)\n",
    "    t2 = query.Term(\"content\", w2)  \n",
    "    pw1=float((searcher.doc_frequency(\"content\", w1)))/doc_count #probabilité de w1\n",
    "    pw2=float((searcher.doc_frequency(\"content\", w2)))/doc_count #probabilité de w2\n",
    "    pocc= float(len(searcher.search(SpanNear(t1, t2, slop=window))))/doc_count     #proba de co-occurence\n",
    "    return math.log((pocc+epsilon)/pw1*pw2)    \n",
    "#NPMI for two words\n",
    "def npmi(searcher, w1, w2,epsilon=0.1, window=20):\n",
    "    doc_count=float(searcher.doc_count())\n",
    "    t1 = query.Term(\"content\", w1)\n",
    "    t2 = query.Term(\"content\", w2) \n",
    "    pocc= float(len(searcher.search(SpanNear(t1, t2, slop=window))))/doc_count  #proba de co-occurence\n",
    "    pmi_w1w2=pmi(searcher, w1, w2,epsilon, window) #pmi entre w1 et w2\n",
    "    return pmi_w1w2 / (- math.log(pocc+epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute p(k|w)\n",
    "def marginal_k_w(doc_dist_topic_k,infolength,word_proba_topic_k):\n",
    "    # p(k|w) est estimée par p(w/k) * somme(p(k/w)*Nd)\n",
    "    return word_proba_topic_k*sum([a*b for a,b in zip(doc_dist_topic_k,infolength)])\n",
    "\n",
    "#compute entropy en w sachant la distrubtion des topics sur les documents et la distributoon du mot w sur les topics\n",
    "def entropy(topic_doc_dist,infolength,topic_dist_word_w):\n",
    "    entropy_w=0\n",
    "    for k in range(len(topic_doc_dist)):\n",
    "        entropy_w+=marginal_k_w(topic_doc_dist[k],infolength,topic_dist_word_w[k])  \n",
    "    return entropy_w\n",
    "\n",
    "# Relevance of word w for topic-id\n",
    "def relevance(topic_id,topic_doc_dist,infolength, topic_dist_word_w):\n",
    "    # divide p(w|k) by entropy\n",
    "    entropy_w=entropy(topic_doc_dist,infolength,topic_dist_word_w)\n",
    "    return topic_dist_word_w[topic_id]/np.exp(entropy_w)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PMI average\n",
    "# calcule la moyenne de pmi pour chaque mot du topic par rapport aux autres mots\n",
    "def pmi_average(searcher,topic_words):\n",
    "    pmi_average=[]\n",
    "    for i in range(len(topic_words)):\n",
    "        w1=topic_words[i]\n",
    "        avg=0\n",
    "        for j in range(len(topic_words)):\n",
    "            if i!=j:\n",
    "                w2=topic_words[j]\n",
    "                avg+=pmi(searcher, w1, w2,epsilon=0.1, window=10)\n",
    "        pmi_average.append(avg/(len(topic_words)-1))\n",
    "    return pmi_average\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupérations des topics (fréquences)\n",
    "Nous récupérons les ensemblents de mots qui constituent les topics, et ce on prenant les TOP N mots de chaque topic par ordre de fréquence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pour un topic donné avoir les topn mots les plus fréquents (bag of ids)\n",
    "def get_topic_terms_list_id(lda,topicid, topn=10):\n",
    "    terms_list=[]\n",
    "    tuples=lda.get_topic_terms(topicid, topn)\n",
    "    for termid, proba in tuples:\n",
    "        terms_list.append(termid)\n",
    "    return terms_list\n",
    "\n",
    "# pour un topic donné, avoir les topn mots les plus fréquents (bag of words)\n",
    "def get_topic_terms_list_alpha(lda,topicid, topn=10):\n",
    "    terms_list=[]\n",
    "    tuples=lda.show_topic(topicid, topn)\n",
    "    for term, proba in tuples:\n",
    "        terms_list.append(term)\n",
    "    return terms_list\n",
    "\n",
    "# afficher la liste des topics (bag of word) selon le topn des mots les plus fréquents\n",
    "def get_list_of_topics_alpha_frequency(lda,num_topics, topn=10):\n",
    "    topic_lists=[]\n",
    "    for i in range(num_topics):\n",
    "        topic_lists.append(get_topic_terms_list_alpha(lda,i, topn=topn))\n",
    "    return topic_lists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupérations des topics (Pertinence)\n",
    "Nous récupérons les ensemblents de mots qui constituent les topics, et ce on prenant les TOP N mots de chaque topic par ordre de pertinence (relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# avoir pour un topic donné, la liste ordonéee des n termes les plus pertinents sur les 50 termes les plus fréquents\n",
    "def get_relevant_topic_terms(lda,topicid,topn, topic_doc_dist, infolength, word_topic_dist):\n",
    "    bag_words=get_topic_terms_list_alpha(lda,topicid,topn=50) #get 50 most frequent words\n",
    "    bag_words_id=get_topic_terms_list_id(lda,topicid,topn=50) #get their ids\n",
    "    m_relevance=[]\n",
    "    for i in range(len(bag_words)):\n",
    "        m_relevance.append(relevance(topicid,topic_doc_dist,infolength, word_topic_dist[bag_words_id[i]]))\n",
    "    sortedRes = sorted(zip(m_relevance, bag_words), key=lambda x: x[0], reverse=True)\n",
    "    return sortedRes[0:topn]\n",
    "\n",
    "# afficher liste des topics (bag of words) selon les topn plus pertinents\n",
    "def get_list_of_topics_alpha_relevance(lda,nu_topics,infolength,topic_doc_dists, term_topic_dists,topn=10):\n",
    "    topic_lists=[]\n",
    "    for i in range(num_topics):\n",
    "        topic_lists.append(get_relevant_topic_terms(lda,i,topn, topic_doc_dists, infolength, term_topic_dists))\n",
    "    return topic_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mesures d'évaluation de cohérence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#================================== Mesures de cohérences pour un seul topic\n",
    "\n",
    "#UCI for one topic\n",
    "def coherence_uci(searcher, topic_words,epsilon=0.1, window=20):\n",
    "    somme=0\n",
    "    N=len(topic_words)\n",
    "    for i in range(N):\n",
    "        for j in range(i+1,N):\n",
    "            somme+=pmi(searcher,topic_words[i], topic_words[j],epsilon, window)\n",
    "    return (somme*2)/(N*(N-1))\n",
    "\n",
    "#U_NPMI for one topic\n",
    "def coherence_npmi(searcher, topic_words,epsilon=0.1, window=10):\n",
    "    somme=0\n",
    "    N=len(topic_words)\n",
    "    for i in range(N):\n",
    "        for j in range(i+1,N):\n",
    "            somme+=npmi(searcher,topic_words[i], topic_words[j],epsilon, window)\n",
    "    return (somme*2)/(N*(N-1))\n",
    "\n",
    "#UMASS for one topic\n",
    "def coherence_umass(searcher,topic_words,epsilon=0.1):\n",
    "    somme=0\n",
    "    N=len(topic_words)\n",
    "    doc_count=float(searcher.doc_count())\n",
    "    \n",
    "    for i in range(1,N):\n",
    "        for j in range(0,i):\n",
    "            t1 = query.Term(\"content\", topic_words[i])\n",
    "            t2 = query.Term(\"content\", topic_words[j]) \n",
    "            pocc= float(len(searcher.search(And([t1,t2]))))/doc_count  #proba de co-occurence                       \n",
    "            pwj=float(len(searcher.search(t2)))/doc_count  #proba du mot le moins rare       \n",
    "            somme+=math.log((pocc+epsilon)/pwj)\n",
    "    return (somme*2)/(N*(N-1))\n",
    "\n",
    "#================================== Mesures de cohérences pour une liste de topics dans l'ordre\n",
    "\n",
    "#UCI\n",
    "def matrixuci(searcher,list_of_topics):\n",
    "    index=[i for i in range(len(list_of_topics))]\n",
    "    coherence=[]\n",
    "    for i in range(len(list_of_topics)):\n",
    "        coherence.append(coherence_uci(searcher, list_of_topics[i],epsilon=0.1, window=10))\n",
    "    sortedRes = sorted(zip(coherence, list_of_topics, index), key=lambda x: x[0], reverse=True)\n",
    "    return sortedRes\n",
    " #5minutes   \n",
    "\n",
    "#NPMI\n",
    "def matrixnpmi(searcher,list_of_topics):\n",
    "    index=[i for i in range(len(list_of_topics))]\n",
    "    coherence=[]\n",
    "    for i in range(len(list_of_topics)):\n",
    "        coherence.append(coherence_npmi(searcher, list_of_topics[i],epsilon=0.1, window=10))\n",
    "    sortedRes = sorted(zip(coherence, list_of_topics, index), key=lambda x: x[0], reverse=True)\n",
    "    return sortedRes\n",
    "    #11 minutes\n",
    "#UMASS\n",
    "def matrixumass(searcher,list_of_topics):\n",
    "    index=[i for i in range(len(list_of_topics))]\n",
    "    coherence=[]\n",
    "    for i in range(len(list_of_topics)):\n",
    "        coherence.append(coherence_umass(searcher, list_of_topics[i],epsilon=0.1))\n",
    "    sortedRes = sorted(zip(coherence, list_of_topics, index), key=lambda x: x[0], reverse=True)\n",
    "    return sortedRes\n",
    "#5 mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests et résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics organisés par fréquence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics_by_frequency=get_list_of_topics_alpha_frequency(lda,num_topics, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UCI\n",
    "ucifreq=matrixuci(searcher,topics_by_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NPMI\n",
    "npmifreq=matrixnpmi(searcher,topics_by_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "umassfreq=matrixumass(searcher,topics_by_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics organisés par pertinence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic_by_relevance=get_list_of_topics_alpha_relevance(lda,num_topics,infolength,topic_doc_dists, term_topic_dists,topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics_by_relevance=[]\n",
    "for topic in dic_by_relevance:\n",
    "    tmp=[]\n",
    "    for proba, word in topic:\n",
    "        tmp.append(word)\n",
    "    topics_by_relevance.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'francais',\n",
       " u'faire',\n",
       " u'france',\n",
       " u'an',\n",
       " u'pouvoir',\n",
       " u'dernier',\n",
       " u'president',\n",
       " u'homme',\n",
       " u'sarkozy',\n",
       " u'ministre']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UCI\n",
    "ucirelev=matrixuci(searcher,topics_by_relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NPMI\n",
    "npmirelev=matrixnpmi(searcher,topics_by_relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labellisation des topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Le mot les plus pertinent est le label\n",
    "liste_labels_pertinence=[]\n",
    "for topic in topics_by_relevance:\n",
    "    tmp=[]\n",
    "    tmp.append(topic[0])\n",
    "    tmp.append(topic)\n",
    "    liste_labels_pertinence.append(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Le mot dont la PMI est la plus elevée est le label\n",
    "liste_labels_pmi=[]\n",
    "for topic in topics_by_relevance:\n",
    "    tmp=[]\n",
    "    tmp.append(topic[np.argmax(pmi_average(searcher, topic))])\n",
    "    tmp.append(topic)\n",
    "    liste_labels_pmi.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
