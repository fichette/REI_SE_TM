{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import os\n",
    "from gensim import corpora, models\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "import re\n",
    "import pickle\n",
    "from pattern.text.fr import parse\n",
    "import math\n",
    "\n",
    "import whoosh.index as index\n",
    "from whoosh.fields import Schema, ID, TEXT\n",
    "from whoosh.index import create_in, open_dir\n",
    "from whoosh.query import Term, SpanNear, And\n",
    "\n",
    "import sys  \n",
    "\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%install_ext autotime.py\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = pickle.load(open(os.getcwd()+\"/5000/texts.obj\", \"rb\"))\n",
    "infopath= pickle.load(open(os.getcwd()+\"/5000/infopath.obj\", \"rb\"))\n",
    "infolength= pickle.load(open(os.getcwd()+\"/5000/infolength.obj\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts] #term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics=50\n",
    "len_vocab=len(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, workers =2,chunksize=10000, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_index(texts, indexPath = \"index\"):\n",
    "    schema = Schema(content=TEXT(stored=True),nid=ID(stored=True))\n",
    "\n",
    "    if not os.path.exists(indexPath):\n",
    "        os.mkdir(indexPath)\n",
    "\n",
    "    index = create_in(indexPath, schema)\n",
    "\n",
    "    writer = index.writer()\n",
    "\n",
    "    for i in range(len(texts)):\n",
    "        writer.add_document(content=texts[i],nid=unicode(i))\n",
    "    writer.commit()\n",
    "create_index(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx=index.open_dir(\"index\")\n",
    "searcher=idx.searcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Matrice Doc Topic Distributions\n",
    "def doc_topic_dists(corpus, lda, num_topics):\n",
    "    doc_topic_dists = []\n",
    "    for doc in range(len(corpus)):\n",
    "        list_topic_proba = [0]*num_topics\n",
    "        temp = lda.get_document_topics(corpus[doc], minimum_probability=0)\n",
    "        for topic, proba in temp:\n",
    "            list_topic_proba[topic]=proba\n",
    "        doc_topic_dists.append(list_topic_proba)\n",
    "    return doc_topic_dists\n",
    "\n",
    "#Matrice Topic Doc Distributions\n",
    "def topic_doc_dists(doc_topic_dists):\n",
    "    topic_doc_dists=[]\n",
    "    for k in range(len(doc_topic_dists[0])):\n",
    "        tmp=[]\n",
    "        for d in range(len(doc_topic_dists)):\n",
    "            tmp.append(doc_topic_dists[d][k])\n",
    "        topic_doc_dists.append(tmp)\n",
    "    return topic_doc_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_topic_dists=doc_topic_dists(corpus, lda, num_topics)\n",
    "topic_doc_dists=topic_doc_dists(doc_topic_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matrice Topic Mot distributions\n",
    "def topic_term_dists(lda, num_topics, len_vocab):\n",
    "    topic_term_dists = []\n",
    "    for topic in range(num_topics):\n",
    "        list_term_proba = [0]*len_vocab\n",
    "        temp = lda.get_topic_terms(topic,topn=len_vocab)\n",
    "        for term, proba in temp:\n",
    "            list_term_proba[term] = proba\n",
    "        topic_term_dists.append(list_term_proba)\n",
    "    return topic_term_dists\n",
    "\n",
    "# matrice Mots Topic distributions\n",
    "def term_topic_dists(topic_term_dists):\n",
    "    term_topic_dists=[]\n",
    "    for w in range(len(topic_term_dists[0])):\n",
    "        tmp=[]\n",
    "        for k in range(len(topic_term_dists)):\n",
    "            tmp.append(topic_term_dists[k][w])\n",
    "        term_topic_dists.append(tmp)\n",
    "    return term_topic_dists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_term_dists=topic_term_dists(lda, num_topics, len_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term_topic_dists=term_topic_dists(topic_term_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PMI for two words\n",
    "def pmi(searcher, w1, w2,epsilon=0.1, window=0):\n",
    "    doc_count=float(searcher.doc_count())\n",
    "    t1 = query.Term(\"content\", w1)\n",
    "    t2 = query.Term(\"content\", w2)  \n",
    "    pw1=float((searcher.doc_frequency(\"content\", w1)))/doc_count\n",
    "    pw2=float((searcher.doc_frequency(\"content\", w2)))/doc_count    \n",
    "    pocc= float(len(searcher.search(SpanNear(t1, t2, slop=window))))/doc_count    \n",
    "    return math.log((pocc+epsilon)/pw1*pw2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NPMI for two words\n",
    "def npmi(searcher, w1, w2,epsilon=0.1, window=20):\n",
    "    doc_count=float(searcher.doc_count())\n",
    "    t1 = query.Term(\"content\", w1)\n",
    "    t2 = query.Term(\"content\", w2) \n",
    "    pocc= float(len(searcher.search(SpanNear(t1, t2, slop=window))))/doc_count \n",
    "    pmi_w1w2=pmi(searcher, w1, w2,epsilon, window)\n",
    "    return pmi_w1w2 / (- math.log(pocc+epsilon))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute p(k|w)\n",
    "def marginal_k_w(doc_dist_topic_k,infolength,word_proba_topic_k):\n",
    "    return word_proba_topic_k*sum([a*b for a,b in zip(doc_dist_topic_k,infolength)])\n",
    "\n",
    "#compute entropy en w\n",
    "def entropy(topic_doc_dist,infolength,topic_dist_word_w):\n",
    "    #compute entropy for word w\n",
    "    entropy_w=0\n",
    "    for k in range(len(topic_doc_dist)):\n",
    "        entropy_w+=marginal_k_w(topic_doc_dist[k],infolength,topic_dist_word_w[k])  \n",
    "    return entropy_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relevance of word w for topic-id\n",
    "def relevance(topic_id,topic_doc_dist,infolength, topic_dist_word_w):\n",
    "    # divide p(w|k) by entropy\n",
    "    entropy_w=entropy(topic_doc_dist,infolength,topic_dist_word_w)\n",
    "    return topic_dist_word_w[topic_id]/np.exp(entropy_w)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pour un topic donné avoir les topn mots les plus fréquents (bag of ids)\n",
    "def get_topic_terms_list_id(lda,topicid, topn=10):\n",
    "    terms_list=[]\n",
    "    tuples=lda.get_topic_terms(topicid, topn)\n",
    "    for termid, proba in tuples:\n",
    "        terms_list.append(termid)\n",
    "    return terms_list\n",
    "\n",
    "# pour un topic donné, avoir les topn mots les plus fréquents (bag of words)\n",
    "def get_topic_terms_list_alpha(lda,topicid, topn=10):\n",
    "    terms_list=[]\n",
    "    tuples=lda.show_topic(topicid, topn)\n",
    "    for term, proba in tuples:\n",
    "        terms_list.append(term)\n",
    "    return terms_list\n",
    "\n",
    "# afficher la liste des topics (bag of word) selon le topn des mots les plus fréquents\n",
    "def get_list_of_topics_alpha_frequency(lda,num_topics, topn=10):\n",
    "    topic_lists=[]\n",
    "    for i in range(num_topics):\n",
    "        topic_lists.append(get_topic_terms_list_alpha(lda,i, topn=topn))\n",
    "    return topic_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# avoir pour un topic donné, la liste ordonéee des n termes les plus fréquents\n",
    "def get_relevant_topic_terms(lda,topicid,topn, topic_doc_dist, infolength, word_topic_dist):\n",
    "    bag_words=get_topic_terms_list_alpha(lda,topicid,topn=50) #get 50 most frequent words\n",
    "    bag_words_id=get_topic_terms_list_id(lda,topicid,topn=50) #get their ids\n",
    "    m_relevance=[]\n",
    "    for i in range(len(bag_words)):\n",
    "        m_relevance.append(relevance(topicid,topic_doc_dist,infolength, word_topic_dist[bag_words_id[i]]))\n",
    "    sortedRes = sorted(zip(m_relevance, bag_words), key=lambda x: x[0], reverse=True)\n",
    "    return sortedRes[0:topn]\n",
    "\n",
    "# afficher liste des topics (bag of words) selon les topn plus importants\n",
    "def get_list_of_topics_alpha_relevance(lda,nu_topics,infolength,topic_doc_dists, term_topic_dists,topn=10):\n",
    "    topic_lists=[]\n",
    "    for i in range(num_topics):\n",
    "        topic_lists.append(get_relevant_topic_terms(lda,i,topn, topic_doc_dists, infolength, term_topic_dists))\n",
    "    return topic_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PMI average\n",
    "def pmi_average(searcher,topic_words):\n",
    "    pmi_average=[]\n",
    "    for i in range(len(topic_words)):\n",
    "        w1=topic_words[i]\n",
    "        avg=0\n",
    "        for j in range(len(topic_words)):\n",
    "            if i!=j:\n",
    "                w2=topic_words[j]\n",
    "                avg+=pmi(searcher, w1, w2,epsilon=0.1, window=10)\n",
    "        pmi_average.append(avg/(len(topic_words)-1))\n",
    "    return pmi_average\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#UCI for one topic\n",
    "def coherence_uci(searcher, topic_words,epsilon=0.1, window=20):\n",
    "    somme=0\n",
    "    N=len(topic_words)\n",
    "    for i in range(N):\n",
    "        for j in range(i+1,N):\n",
    "            somme+=pmi(searcher,topic_words[i], topic_words[j],epsilon, window)\n",
    "    return (somme*2)/(N*(N-1))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#U_NPMI for one topic\n",
    "def coherence_npmi(searcher, topic_words,epsilon=0.1, window=10):\n",
    "    somme=0\n",
    "    N=len(topic_words)\n",
    "    for i in range(N):\n",
    "        for j in range(i+1,N):\n",
    "            somme+=npmi(searcher,topic_words[i], topic_words[j],epsilon, window)\n",
    "    return (somme*2)/(N*(N-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#UMASS for one topic\n",
    "def coherence_umass(searcher,topic_words,epsilon=0.1):\n",
    "    somme=0\n",
    "    N=len(topic_words)\n",
    "    doc_count=float(searcher.doc_count())\n",
    "    \n",
    "    for i in range(1,N):\n",
    "        for j in range(0,i):\n",
    "            t1 = query.Term(\"content\", topic_words[i])\n",
    "            t2 = query.Term(\"content\", topic_words[j]) \n",
    "            pocc= float(len(searcher.search(And([t1,t2]))))/doc_count \n",
    "                        \n",
    "            pwj=float(len(searcher.search(t2)))/doc_count\n",
    "            \n",
    "            somme+=math.log((pocc+epsilon)/pwj)\n",
    "    return (somme*2)/(N*(N-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrixuci(searcher,list_of_topics):\n",
    "    index=[i for i in range(len(list_of_topics))]\n",
    "    coherence=[]\n",
    "    for i in range(len(list_of_topics)):\n",
    "        coherence.append(coherence_uci(searcher, list_of_topics[i],epsilon=0.1, window=10))\n",
    "    sortedRes = sorted(zip(coherence, list_of_topics, index), key=lambda x: x[0], reverse=True)\n",
    "    return sortedRes\n",
    " #5minutes   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrixnpmi(searcher,list_of_topics):\n",
    "    index=[i for i in range(len(list_of_topics))]\n",
    "    coherence=[]\n",
    "    for i in range(len(list_of_topics)):\n",
    "        coherence.append(coherence_npmi(searcher, list_of_topics[i],epsilon=0.1, window=10))\n",
    "    sortedRes = sorted(zip(coherence, list_of_topics, index), key=lambda x: x[0], reverse=True)\n",
    "    return sortedRes\n",
    "    #11 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrixumass(searcher,list_of_topics):\n",
    "    index=[i for i in range(len(list_of_topics))]\n",
    "    coherence=[]\n",
    "    for i in range(len(list_of_topics)):\n",
    "        coherence.append(coherence_umass(searcher, list_of_topics[i],epsilon=0.1))\n",
    "    sortedRes = sorted(zip(coherence, list_of_topics, index), key=lambda x: x[0], reverse=True)\n",
    "    return sortedRes\n",
    "#5 mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
