\section{Mutual Information}

Outline: 

\begin{enumerate}
\item Définir la problématique: mesurer à quel point le topic modeling est bon

\item Présenter quelques méthodes qui existent pour évaluer les performances du topic modeling

\item Définir Mutual Information/ Pointwise Mutual Information

\item Comment nous allons utiliser la PMI pour 
	\begin{itemize}
		\item Evaluer la cohérence des mots dans un topic
		\item Evaluer combien un mot distingue un topic
		\item Labeliser les topics avec la PMI
	\end{itemize}

\item Implémentation de la PMI
\item Tests et Résultats 
	
	
\subsection{brouillon}

\textbf{Topic Coherence} measures score a single topic by measuring the degree of semantic similarity between
high scoring words in the topic.  These measure-
ments help distinguish between topics that are se-
mantically interpretable topics and topics that are artifacts of statistical inference \cite{Stevens:2012}

\textbf{Mutual information} measures how much information – in the information-
theoretic sense – a term contains about the class. If a term’s distribution is
the same in the class as it is in the collection as a whole, then I (U; C) =
0. MI reaches its maximum value if the term is a perfect indicator for class
membership, that is, if the term is present in a document if and only if the
document is in the class. \cite{schutze2008introduction}


 To apply topic modeling to solve
 real world problems, we need an easy way to identify “junk”
 topics (e.g. banana sky canoe furious), which may be sta-
 tistically well founded, but of no use to end users. 
 
 
 The intuition behind the scoring model comes from the
 idea that the coherence of a set of ten words implies re-
 latedness of most or all pairs of words taken from the set.
 This leads to the idea of a scoring model based on word
 association between pairs of words, for all word pairs in a
 topic. But instead of using the collection itself to measure
 word association — which could reinforce noise or unusual
 word statistics — we use a large external text data source.
 Specifically, we measured co-occurrence of word pairs from
 large external text datasets such as all articles from English
 Wikipedia,


\textbf{C\_NPMI} is an enhanced version of the C\_UCI coherence using the normalized pointwise mutual information (NPMI) instead of the pointwise mutual information (PMI).

Proposed in
N. Aletras and M. Stevenson: Evaluating topic coherence using distributional semantics. In Proceedings of the 10th International Conference on Computational Semantics (IWCS'13) Long Papers, pages 13-22, 2013.


\textbf{C\_UCI} is a coherence that is based on a sliding window and the pointwise mutual information (PMI) of all word pairs of the given top words.

The word cooccurrence counts are derived using a sliding window with the size 10. For every word pair the PMI is calculated. The arithmetic mean of the PMI values is the result of this coherence. (Note that in the original publication only the sum of these values is calculated)

Proposed in
D. Newman, J. H. Lau, K. Grieser, and T. Baldwin: Automatic evaluation of topic coherence. In Human Language Technologies: The 2010 Annual Conferenceof the North American Chapter of the Association for Computational Linguistics, pages 100-108. Association for Computational Linguistics, 2010.


\textbf{C\_UMass} is based on document cooccurrence counts, a one-preceding segmentation and a logarithmic conditional probability as confirmation measure.

The main idea of this coherence is that the occurrence of every top word should be supported by every top preceding top word. Thus, the probability of a top word to occur should be higher if a document already contains a higher order top word of the same topic. Therefore, for every word the logarithm of its conditional probability is calculated using every other top word that has a higher order in the ranking of top words as condition. The probabilities are derived using document cooccurrence counts. The single conditional probabilities are summarized using the arithmetic mean. (Note that in the original publication only the sum of these values is calculated)

Proposed in
D. Mimno, H. M. Wallach, E. Talley, M. Leenders, and A. McCallum: Optimizing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 262-272. Association for Computational Linguistics, 2011.
\end{enumerate}
